{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO Demo 3\n",
    "\n",
    "## Let's detect objects *asynchronously* in real time using a cam feed!\n",
    "\n",
    "Continuing on from the last demo, let's build upon what we learned and detect things in real time anhe d do things asynchronously! Spreading inference tasks upon multiple threads allows for us to use the full capability of our processor. While we wait on the inference task that we want to perhaps display, other inference tasks work in the background.\n",
    "\n",
    "From the beginning, let's create functions that will help us out later. After we import all the libraries/packages we need, let's define our first function which will pre-process our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required packages\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openvino.inference_engine import IENetwork\n",
    "from openvino.inference_engine import IEPlugin\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# CHANGE AS NEEDED\n",
    "OS = 'linux'\n",
    "dev = 'MYRIAD' # Change to MYRIAD if Intel NCS 2 plugged in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(obj_frame, input_shape):\n",
    "    n, c, h, w = input_shape\n",
    "    obj_in_frame = cv2.resize(obj_frame, (w, h))\n",
    "    obj_in_frame = obj_in_frame.transpose((2, 0, 1))\n",
    "    obj_in_frame = obj_in_frame.reshape((n, c, h, w))\n",
    "    \n",
    "    return {\n",
    "        'blob' : obj_in_frame, \n",
    "        'frame': obj_frame, \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define our second function which will return our net and useful information about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_nn(path_to_xml, path_to_bin, dev, OS, reqs_num):\n",
    "    obj_net = IENetwork(model=path_to_xml, weights=path_to_bin)\n",
    "    input_layer = next(iter(obj_net.inputs))\n",
    "    output_layer = next(iter(obj_net.outputs))\n",
    "    net_shape = obj_net.inputs[input_layer].shape\n",
    "    \n",
    "    ext = None\n",
    "    if OS.lower() == 'windows':\n",
    "        ext = 'C:\\\\Users\\\\freyes\\\\Documents\\\\Intel\\\\OpenVINO\\\\inference_engine_samples_build\\\\intel64\\\\Release\\\\cpu_extension.dll'\n",
    "    else:\n",
    "        ext = '/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so'\n",
    "\n",
    "    obj_plugin = IEPlugin(device=dev)\n",
    "    if dev.lower() == 'cpu':\n",
    "        obj_plugin.add_cpu_extension(ext)\n",
    "        \n",
    "    # ASYNC EDIT: Added request number instead of 1, like in sync demo\n",
    "    obj_exec_net = obj_plugin.load(network=obj_net, num_requests=reqs_num)\n",
    "    return {'net': obj_exec_net, 'input_layer': input_layer,\n",
    "            'output_layer': output_layer, 'shape': net_shape}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define our third function which will process our data and draw the bounding box around the image we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bb(obj_det, obj_frame):\n",
    "    initial_w = obj_frame.shape[1]\n",
    "    initial_h = obj_frame.shape[0]\n",
    "    green = (0, 255, 0)\n",
    "\n",
    "    for obj in obj_det[0][0]:\n",
    "        # Draw only objects when probability more than specified threshold\n",
    "        if obj[2] > 0.5:\n",
    "            xmin = int(obj[3] * initial_w)\n",
    "            ymin = int(obj[4] * initial_h)\n",
    "            xmax = int(obj[5] * initial_w)\n",
    "            ymax = int(obj[6] * initial_h)\n",
    "            class_id = int(obj[1])\n",
    "\n",
    "            # Draw box and label\\class_id\n",
    "            color = (min(class_id * 12.5, 255), min(class_id * 7, 255), min(class_id * 5, 255))\n",
    "            cv2.rectangle(obj_frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "            cv2.putText(obj_frame, str(class_id), (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.8, green, 2, cv2.LINE_AA)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's put the pieces together! In this example, more variables are added to keep track of async requests and their associated video frames. Over time, frames will fill in all requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot load library '/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so': 126 from cwd: C:\\Users\\freyes\\projects\\wine_detector",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bc93e5bfb759>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-bc93e5bfb759>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(OS, dev)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# net, input_layer, output_layer, shape; added reqs_num\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mnet_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_objxml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_objbin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CPU'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreqs_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mobj_exec_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'net'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'shape'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6b8eb07c86ea>\u001b[0m in \u001b[0;36mconstruct_nn\u001b[1;34m(path_to_xml, path_to_bin, dev, OS, reqs_num)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mobj_plugin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIEPlugin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mobj_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_cpu_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# ASYNC EDIT: Added request number instead of 1, like in sync demo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mie_api.pyx\u001b[0m in \u001b[0;36mopenvino.inference_engine.ie_api.IEPlugin.add_cpu_extension\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mie_api.pyx\u001b[0m in \u001b[0;36mopenvino.inference_engine.ie_api.IEPlugin.add_cpu_extension\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot load library '/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so': 126 from cwd: C:\\Users\\freyes\\projects\\wine_detector"
     ]
    }
   ],
   "source": [
    "# Object Detection Section but Async!\n",
    "def main(OS, dev):\n",
    "    # CHANGE THIS AS NEEDED\n",
    "    reqs_num = 4 # request number that we pass in\n",
    "    \n",
    "    # ASYNC VARIABLES FOR MANAGING TASKS\n",
    "    cur_request_id = reqs_num - 2\n",
    "    next_request_id = reqs_num - 1\n",
    "    requests_arr = [x for x in range(reqs_num)]\n",
    "    frame_dicts = [None for x in range(reqs_num)]\n",
    "    \n",
    "    path_to_objxml = None\n",
    "    path_to_objbin = None\n",
    "    fp = 'fp32' if dev.lower() == 'cpu' else 'fp16'\n",
    "    if OS.lower() == 'linux':\n",
    "        path_to_objxml = 'gesture_optimized/' + fp + '/frozen_inference_graph.xml'\n",
    "        path_to_objbin = 'gesture_optimized/' + fp + '/frozen_inference_graph.bin'\n",
    "    elif os.lower() == 'windows':\n",
    "        path_to_objxml = 'gesture_optimized\\\\' + fp + '\\\\frozen_inference_graph.xml'\n",
    "        path_to_objbin = 'gesture_optimized\\\\' + fp + '\\\\frozen_inference_graph.bin'\n",
    "    else:\n",
    "        print(\"Need to have either linux or windows!\")\n",
    "        return\n",
    "    \n",
    "    # net, input_layer, output_layer, shape; added reqs_num\n",
    "    net_dict = construct_nn(path_to_objxml, path_to_objbin, 'CPU', OS, reqs_num)\n",
    "    obj_exec_net = net_dict['net']\n",
    "    input_shape = net_dict['shape']\n",
    "    input_layer = net_dict['input_layer']\n",
    "    output_layer = net_dict['output_layer']\n",
    "    \n",
    "    vs = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Adding a frame to the frame_dicts array\n",
    "    # Corresponds to request id in requests_arr\n",
    "    ret, pre_vframe = vs.read()\n",
    "    frame_dicts[cur_request_id] = pre_processing(pre_vframe, input_shape)\n",
    "    vframe = frame_dicts[cur_request_id]['frame']\n",
    "    \n",
    "    print(\"Click on the window and press q to exit the application.\")\n",
    "    while True:\n",
    "        start = time.time()\n",
    "        \n",
    "        # Populating frame_dictss array with next video frame dict (next_image_dict)\n",
    "        ret, next_vframe = vs.read()\n",
    "        next_image_dict = pre_processing(next_vframe, input_shape)\n",
    "        frame_dicts[next_request_id] = next_image_dict\n",
    "        \n",
    "        # Now will populate requests with next input blob, inference done in background\n",
    "        next_vframe = next_image_dict['frame']\n",
    "        next_inpBlob = next_image_dict['blob']\n",
    "        obj_res = obj_exec_net.start_async(request_id=next_request_id, \n",
    "                                     inputs={input_layer: next_inpBlob})\n",
    "        \n",
    "        # Conditional will be used to process finished inference task\n",
    "        if obj_exec_net.requests[cur_request_id].wait(-1) == 0:\n",
    "            obj_res = obj_exec_net.requests[cur_request_id].outputs\n",
    "            obj_detections = obj_res[output_layer]\n",
    "            draw_bb(obj_detections, vframe)\n",
    "        \n",
    "        # Showing stats\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        stamp = time.time() - start \n",
    "        cv2.putText(vframe, \"Time: \" + str(stamp), (30, 30), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(vframe, \"FPS: \" + str(1/stamp), (30, 60), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Showing current request's associated frame that was post processed\n",
    "        cv2.imshow(\"Frame (Async Mode)\", vframe)\n",
    "\n",
    "        # Switching requests and frames to focus on during next render\n",
    "        cur_request_id, next_request_id = next_request_id, (next_request_id + 1) % reqs_num\n",
    "        vframe = next_vframe\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    " \n",
    "    # do a bit of cleanup\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "main(OS, dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "We now know how to detect objects in real time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
