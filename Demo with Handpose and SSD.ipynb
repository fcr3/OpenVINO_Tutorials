{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO Demo: Multiple Networks in Action\n",
    "\n",
    "### Here is a demo using mutliple networks!\n",
    "\n",
    "We are going to be combining what we know in this demo. Please look at the other walkthroughs if the code is hard to follow.\n",
    "\n",
    "**Note: this demo will not work with the Intel NCS 2 due to limited memory resources.**\n",
    "\n",
    "First, let's import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "import time\n",
    "\n",
    "User = 'fcrey'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the necessary functions in order to set up the network, pre-process the input dat, and post-process dat from the output layer and draw bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for object detection\n",
    "\n",
    "def setup_obj_net(precision, device_choice, OS, ie):\n",
    "    path_to_objxml = path_to_objbin = None\n",
    "    if OS.lower() == 'windows':\n",
    "        path_to_objxml = 'gesture_optimized\\\\' + precision + '\\\\frozen_inference_graph.xml'\n",
    "        path_to_objbin = 'gesture_optimized\\\\' + precision + '\\\\frozen_inference_graph.bin'\n",
    "    else:\n",
    "        path_to_objxml = './gesture_optimized/' + precision + '/frozen_inference_graph.xml'\n",
    "        path_to_objbin = './gesture_optimized/' + precision + '/frozen_inference_graph.bin'\n",
    "\n",
    "    net = IENetwork(model=path_to_objxml, weights=path_to_objbin)\n",
    "    input_layer = next(iter(net.inputs))\n",
    "    output_layer = next(iter(net.outputs))\n",
    "    input_shape = net.inputs[input_layer].shape\n",
    "    \n",
    "    ext = None\n",
    "    if OS.lower() == 'windows':\n",
    "        ext = 'C:\\\\Users\\\\' + User + '\\\\Documents\\\\Intel\\\\OpenVINO\\\\inference_engine_samples_build\\\\intel64\\\\Release\\\\cpu_extension.dll'\n",
    "    else:\n",
    "        ext = '/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so'\n",
    "    if device_choice.lower() == 'cpu':\n",
    "        ie.add_extension(ext, device_name=device_choice)\n",
    "    obj_exec_net = ie.load_network(network=net, device_name=device_choice, num_requests=1)\n",
    "    \n",
    "    return {\n",
    "        'net': obj_exec_net, \n",
    "        'input_layer': input_layer,\n",
    "        'output_layer': output_layer,\n",
    "        'input_shape': input_shape\n",
    "    }\n",
    "\n",
    "def pre_obj_processing(obj_frame, input_shape):\n",
    "    n, c, h, w = input_shape\n",
    "    obj_in_frame = cv2.resize(obj_frame, (w, h))\n",
    "    obj_in_frame = obj_in_frame.transpose((2, 0, 1))\n",
    "    obj_in_frame = obj_in_frame.reshape((n, c, h, w))\n",
    "    \n",
    "    return {\n",
    "        'blob' : obj_in_frame, \n",
    "        'frame': obj_frame, \n",
    "    }\n",
    "\n",
    "def draw_bb(obj_det, frame):\n",
    "    i_w = frame.shape[1]\n",
    "    i_h = frame.shape[0]\n",
    "    drawn = False\n",
    "    \n",
    "    proposals = []\n",
    "    \n",
    "    for obj in obj_det[0][0]:\n",
    "        if obj[2] > 0.5:\n",
    "            proposals.append(obj)\n",
    "    if len(proposals) == 0:\n",
    "        return {'status': False, 'miniframe': None, 'frame': frame, 'class': 8, 'offset': None}\n",
    "    \n",
    "    proposed_obj = max(proposals, key=lambda x: x[2])\n",
    "    xmin = int(proposed_obj[3] * i_w)\n",
    "    ymin = int(proposed_obj[4] * i_h)\n",
    "    xmax = int(proposed_obj[5] * i_w)\n",
    "    ymax = int(proposed_obj[6] * i_h)\n",
    "    class_id = int(proposed_obj[1])\n",
    "    green = (0, 255, 0)\n",
    "    cv2.putText(frame, str(class_id), (xmin, ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.8, green, 2, cv2.LINE_AA )\n",
    "    cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), green, 2)\n",
    "    \n",
    "    xmid = (xmin + xmax) / 2\n",
    "    ymid = (ymin + ymax) / 2\n",
    "    \n",
    "    xmin = int(xmid - 149)\n",
    "    xmax = int(xmid + 150)\n",
    "    ymin = int(ymid - 149)\n",
    "    ymax = int(ymid + 150)\n",
    "    \n",
    "    if ymin >= i_h or xmin >= i_w or ymin < 0 or xmin < 0:\n",
    "        return {'status': False, 'miniframe': None, 'frame': frame, 'class': 8, 'offset': None}\n",
    "    \n",
    "    # Draw box and label\\class_id\n",
    "    cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255, 255, 255), 2)\n",
    "    \n",
    "    return {'status': True, 'miniframe': frame[ymin:ymax, xmin:xmax], \n",
    "            'frame': frame, 'class': class_id, 'offset': (xmin, ymin)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will defined all the necessary functions for loading the optimized HandPose net, pre-processing input data, and post-processing data to draw handkeypoints on a frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for hand pose detection\n",
    "\n",
    "def setup_net(shape, precision, device_choice, OS, ie):\n",
    "    path_to_hpxml = path_to_hpbin = None\n",
    "    if OS.lower() == 'windows':\n",
    "        path_to_hpxml = 'handpose_optimized\\\\' + precision + '\\\\pose_iter_102000.xml'\n",
    "        path_to_hpbin = 'handpose_optimized\\\\' + precision + '\\\\pose_iter_102000.bin'\n",
    "    else:\n",
    "        path_to_hpxml = './handpose_optimized/' + precision + '/pose_iter_102000.xml'\n",
    "        path_to_hpbin = './handpose_optimized/' + precision + '/pose_iter_102000.bin'\n",
    "\n",
    "    # Set up network for inference\n",
    "    net = IENetwork(model=path_to_hpxml, weights=path_to_hpbin)\n",
    "\n",
    "    # Based on prototxt, original input: [2, 3, 368, 368]\n",
    "    input_layer = next(iter(net.inputs))\n",
    "    output_layer = next(iter(net.outputs))\n",
    "    n, c, _, _ = net.inputs[input_layer].shape\n",
    "    net.reshape({input_layer: (n, c, shape[1], shape[0])})\n",
    "    \n",
    "    exec_net = ie.load_network(network=net, device_name=device_choice, num_requests=1)\n",
    "    return {'net': exec_net, 'input_layer': input_layer, 'output_layer': output_layer}\n",
    "\n",
    "def pre_hp_processing(frame, shape):\n",
    "    try:\n",
    "        inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, shape, (0, 0, 0), swapRB=True, crop=False)\n",
    "\n",
    "        return {'blob' : inpBlob, 'frame': frame, 'status': True}\n",
    "    except:\n",
    "        return {'blob' : None, 'frame' : None, 'status': False}\n",
    "\n",
    "def draw_skeleton(output, frame, offset, big_frame, class_id):\n",
    "    points = []\n",
    "    frameCopy = np.copy(frame)\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "    nPoints = 22\n",
    "    POSE_PAIRS = [[0,1],[1,2],[2,3],[3,4],[0,5],[5,6],[6,7],[7,8],[0,9],[9,10],[10,11],[11,12],[0,13],[13,14],[14,15],[15,16],[0,17],[17,18],[18,19],[19,20]]\n",
    "    threshold = 0.15\n",
    "\n",
    "    for i in range(nPoints):\n",
    "        # confidence map of corresponding body's part.\n",
    "        probMap = output[0, i, :, :]\n",
    "        probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n",
    "\n",
    "        # Find global maxima of the probMap.\n",
    "        minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "        if prob > threshold :\n",
    "            coord = (int(point[0]) + offset[0], int(point[1]) + offset[1])\n",
    "            color = (0, 0, 255)\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(big_frame, \"{}\".format(i), coord, font, .8, color, 2, lineType=cv2.LINE_AA)\n",
    "\n",
    "            # Add the point to the list if the probability is greater than the threshold\n",
    "            points.append(coord)\n",
    "        else :\n",
    "            points.append(None)\n",
    "\n",
    "    # Draw Skeleton\n",
    "    for pair in POSE_PAIRS:\n",
    "        partA = pair[0]\n",
    "        partB = pair[1]\n",
    "\n",
    "        if points[partA] and points[partB]:\n",
    "            cv2.line(big_frame, points[partA], points[partB], (0, 255, 255), 2)\n",
    "    \n",
    "    if class_id == 3 or class_id == 4:\n",
    "        # Index Finger Points\n",
    "        vector_points = [points[0], points[5], points[6], points[7], points[8]]\n",
    "        for i in range(4):\n",
    "            if vector_points[i] != None and vector_points[i + 1] != None:\n",
    "                if vector_points[i][0] < vector_points[i + 1][0]:\n",
    "                    cv2.putText(big_frame, str(3), offset, font, .8, color, 2, lineType=cv2.LINE_AA)\n",
    "                else:\n",
    "                    cv2.putText(big_frame, str(4), offset, font, .8, color, 2, lineType=cv2.LINE_AA)\n",
    "                return\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the pieces together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD + Hand Pose Estimation\n",
    "def main():\n",
    "    ie = IECore()\n",
    "    OS = 'windows'\n",
    "    device = 'CPU'\n",
    "    precision = 'fp32' if device.lower() == 'cpu' else 'fp16'\n",
    "    \n",
    "    # object detection net\n",
    "    net_dict = setup_obj_net(precision, device, OS, ie)\n",
    "    obj_exec_net = net_dict['net']\n",
    "    obj_input_shape = net_dict['input_shape']\n",
    "    \n",
    "    # hand pose estimation net\n",
    "    hp_shape = (229, 229)\n",
    "    hp_net_dict = setup_net(hp_shape, precision, device, OS, ie)\n",
    "    hp_net = hp_net_dict['net']\n",
    "    hp_input_layer = hp_net_dict['input_layer']\n",
    "    hp_output_layer = hp_net_dict['output_layer']\n",
    "    \n",
    "    vs = cv2.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "        start = time.time()\n",
    "        # obj detection routine\n",
    "        ret, vframe = vs.read()\n",
    "        # vframe = cv2.cvtColor(vframe, cv2.COLOR_BGR2GRAY)\n",
    "        image_dict = pre_obj_processing(vframe, obj_input_shape)\n",
    "        obj_inpBlob = image_dict['blob']\n",
    "\n",
    "        obj_res = obj_exec_net.infer({'image_tensor': obj_inpBlob})\n",
    "        obj_det = obj_res['DetectionOutput']\n",
    "        \n",
    "        proposed_frame = draw_bb(obj_det, image_dict['frame'])\n",
    "        class_id = proposed_frame['class']\n",
    "        big_frame = proposed_frame['frame']\n",
    "        \n",
    "        if proposed_frame['status']:\n",
    "            new_frame = proposed_frame['miniframe']\n",
    "            offset = proposed_frame['offset']\n",
    "            \n",
    "            # hand pose estimation routine\n",
    "            sub_image_dict = pre_hp_processing(new_frame, hp_shape)\n",
    "            if not sub_image_dict['status']:\n",
    "                cv2.imshow(\"Hand Key Points\", big_frame)\n",
    "                continue\n",
    "            \n",
    "            hs_inpBlob = sub_image_dict['blob']\n",
    "            hp_res = hp_net_dict['net'].infer({hp_input_layer: hs_inpBlob})\n",
    "            hp_output = hp_res[hp_output_layer]\n",
    "            \n",
    "            draw_skeleton(hp_output, sub_image_dict['frame'], offset, big_frame, class_id)\n",
    "            \n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            stamp = time.time() - start \n",
    "            cv2.putText(vframe, \"Time: \" + str(stamp), (30, 30), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(vframe, \"FPS: \" + str(1/stamp), (30, 60), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.imshow(\"Hand Key Points\", big_frame)\n",
    "        else:\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            stamp = time.time() - start \n",
    "            cv2.putText(vframe, \"Time: \" + str(stamp), (30, 30), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(vframe, \"FPS: \" + str(1/stamp), (30, 60), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.imshow(\"Hand Key Points\", big_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    " \n",
    "    # do a bit of cleanup\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, I have a demo here using just the handpose model. This hopefully illustrates how the utilization of two networks can speed up inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand Pose Estimation Demo\n",
    "def main():\n",
    "    ie = IECore()\n",
    "    OS = 'windows'\n",
    "    device = 'CPU'\n",
    "    precision = 'fp32' if device.lower() == 'cpu' else 'fp16'\n",
    "    \n",
    "    vs = cv2.VideoCapture(0)\n",
    "    \n",
    "    ret, vframe = vs.read()\n",
    "    aspect_ratio = vframe.shape[1]/vframe.shape[0]\n",
    "    inHeight = 268\n",
    "    inWidth = int(((aspect_ratio*inHeight)*8)//8)\n",
    "    image_dict = pre_hp_processing(vframe, (inWidth, inHeight))\n",
    "    net_dict = setup_net((inWidth, inHeight), precision, device, OS, ie)\n",
    "    \n",
    "    while True:\n",
    "        start = time.time()\n",
    "        ret, vframe = vs.read()\n",
    "        image_dict = pre_hp_processing(vframe, (inWidth, inHeight))\n",
    "        inpBlob = image_dict['blob']\n",
    "\n",
    "        net = net_dict['net']\n",
    "        input_layer = net_dict['input_layer']\n",
    "        output_layer = net_dict['output_layer']\n",
    "\n",
    "        res = net_dict['net'].infer({input_layer: inpBlob})\n",
    "        hp_output = res[output_layer]\n",
    "        \n",
    "        draw_skeleton(hp_output, image_dict['frame'], (0, 0), vframe, None)\n",
    "        \n",
    "        stamp = time.time() - start \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(vframe, \"Time: \" + str(stamp), (30, 30), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(vframe, \"FPS: \" + str(1/stamp), (30, 60), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Frame\", image_dict['frame'])\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    " \n",
    "    # do a bit of cleanup\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
